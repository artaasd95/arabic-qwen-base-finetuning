#!/usr/bin/env python3
"""
Simple Inference Script for Arabic Qwen Models
Allows users to quickly test their trained models
"""

import json
import argparse
from pathlib import Path
from typing import List, Dict

def load_available_models() -> Dict[str, Dict]:
    """Load available trained models"""
    model_paths_file = Path("outputs/model_paths.json")
    
    if not model_paths_file.exists():
        print("âŒ No trained models found. Please run training first.")
        return {}
    
    with open(model_paths_file, 'r', encoding='utf-8') as f:
        model_paths = json.load(f)
    
    available_models = {}
    for i, model_info in enumerate(model_paths):
        model_name = model_info['model_name']
        available_models[str(i+1)] = {
            'name': model_name,
            'path': model_info['path'],
            'method': model_info['method'],
            'dataset': model_info['dataset'],
            'samples': model_info['samples_trained']
        }
    
    return available_models

def display_models(models: Dict[str, Dict]):
    """Display available models"""
    print("\nğŸ“‹ Available Models:")
    print("====================")
    
    for key, model_info in models.items():
        print(f"{key}. {model_info['name']}")
        print(f"   Method: {model_info['method']}")
        print(f"   Dataset: {model_info['dataset']}")
        print(f"   Samples: {model_info['samples']:,}")
        print(f"   Path: {model_info['path']}")
        print()

def mock_inference(model_info: Dict, prompt: str, task_type: str) -> str:
    """Mock inference function (replace with real implementation)"""
    model_name = model_info['name']
    method = model_info['method']
    
    # Mock responses based on method and task type
    if task_type == "chat":
        if "SFT" in method:
            return "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬ SFT Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ù…Ø³Ø§Ø¹Ø¯ØªÙƒ Ø§Ù„ÙŠÙˆÙ…ØŸ"
        elif "DPO" in method:
            return "Ø£Ù‡Ù„Ø§Ù‹ ÙˆØ³Ù‡Ù„Ø§Ù‹! Ø£Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬ DPO Ù…Ø­Ø³Ù‘Ù† Ù„Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø§Øª Ø§Ù„Ù…ÙØ¶Ù„Ø©. Ø£Ø³Ø¹Ø¯ Ø¨Ø®Ø¯Ù…ØªÙƒ."
        elif "KTO" in method:
            return "Ù…Ø±Ø­Ø¨Ø§Ù‹ Ø¨Ùƒ! Ø£Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬ KTO Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ø¨ØªÙ‚Ù†ÙŠØ© ÙƒØ§Ù†Ù…Ø§Ù†-ØªÙØ±Ø³ÙƒÙŠ. ÙƒÙŠÙ Ø£Ø³Ø§Ø¹Ø¯ÙƒØŸ"
        elif "IPO" in method:
            return "Ø£Ù‡Ù„Ø§Ù‹! Ø£Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬ IPO Ø§Ù„Ø³Ø±ÙŠØ¹ ÙˆØ§Ù„ÙØ¹Ø§Ù„. Ù…Ø§ Ø§Ù„Ø°ÙŠ ØªØ­ØªØ§Ø¬ Ø¥Ù„ÙŠÙ‡ØŸ"
        elif "CPO" in method:
            return "Ù…Ø±Ø­Ø¨Ø§Ù‹! Ø£Ù†Ø§ Ù†Ù…ÙˆØ°Ø¬ CPO Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù„Ù„ÙÙ‡Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚. ÙƒÙŠÙ ÙŠÙ…ÙƒÙ†Ù†ÙŠ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯Ø©ØŸ"
    
    elif task_type == "qa":
        return "Ù‡Ø°Ù‡ Ø¥Ø¬Ø§Ø¨Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙÙŠ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ø³ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙˆØªÙ‚Ø¯ÙŠÙ… Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù†Ø§Ø³Ø¨Ø©."
    
    elif task_type == "instruction":
        return "ØªÙ… ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø¨Ù†Ø¬Ø§Ø­. Ù‡Ø°Ù‡ Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© ØªÙˆØ¶Ø­ ÙƒÙŠÙÙŠØ© Ø¹Ù…Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©."
    
    elif task_type == "generation":
        return f"Ù‡Ø°Ø§ Ù†Øµ Ù…ÙÙˆÙ„Ø¯ Ù…Ù† Ù†Ù…ÙˆØ°Ø¬ {method}. Ø§Ù„Ù†Øµ Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ Ø³ÙŠÙƒÙˆÙ† Ø£ÙƒØ«Ø± ØªÙ…Ø§Ø³ÙƒØ§Ù‹ ÙˆØµÙ„Ø© Ø¨Ø§Ù„Ù…ÙˆØ¶ÙˆØ¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨."
    
    return "Ø§Ø³ØªØ¬Ø§Ø¨Ø© ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù…Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬."

def interactive_mode(model_info: Dict):
    """Interactive chat mode"""
    print(f"\nğŸ¤– Interactive Mode - {model_info['name']}")
    print("Type 'quit' to exit, 'help' for commands")
    print("=" * 50)
    
    while True:
        try:
            user_input = input("\nğŸ‘¤ You: ").strip()
            
            if user_input.lower() in ['quit', 'exit', 'Ø®Ø±ÙˆØ¬']:
                print("ğŸ‘‹ Goodbye!")
                break
            
            if user_input.lower() == 'help':
                print("\nğŸ“š Available commands:")
                print("  - Type any message to chat")
                print("  - 'quit' or 'exit' to leave")
                print("  - 'help' for this message")
                continue
            
            if not user_input:
                continue
            
            # Mock inference
            response = mock_inference(model_info, user_input, "chat")
            print(f"ğŸ¤– {model_info['method']}: {response}")
            
        except KeyboardInterrupt:
            print("\nğŸ‘‹ Goodbye!")
            break
        except Exception as e:
            print(f"âŒ Error: {e}")

def single_inference(model_info: Dict, prompt: str, task_type: str):
    """Single inference mode"""
    print(f"\nğŸ” Single Inference - {model_info['name']}")
    print(f"Task Type: {task_type}")
    print("=" * 50)
    
    print(f"ğŸ“ Input: {prompt}")
    
    # Mock inference
    response = mock_inference(model_info, prompt, task_type)
    print(f"ğŸ¤– Output: {response}")
    
    # Show model info
    print(f"\nğŸ“Š Model Info:")
    print(f"  Method: {model_info['method']}")
    print(f"  Dataset: {model_info['dataset']}")
    print(f"  Training Samples: {model_info['samples']:,}")

def benchmark_mode(models: Dict[str, Dict], prompt: str):
    """Benchmark multiple models with the same prompt"""
    print(f"\nâš¡ Benchmark Mode")
    print(f"Testing prompt: {prompt}")
    print("=" * 50)
    
    for key, model_info in models.items():
        print(f"\n{key}. {model_info['method']} ({model_info['dataset']})")
        response = mock_inference(model_info, prompt, "generation")
        print(f"   Response: {response}")

def main():
    parser = argparse.ArgumentParser(description="Arabic Qwen Model Inference")
    parser.add_argument("--model", type=str, help="Model number to use (1, 2, 3, ...)")
    parser.add_argument("--prompt", type=str, help="Input prompt for inference")
    parser.add_argument("--task", type=str, choices=["chat", "qa", "instruction", "generation"], 
                       default="chat", help="Task type")
    parser.add_argument("--interactive", action="store_true", help="Start interactive mode")
    parser.add_argument("--benchmark", action="store_true", help="Benchmark all models")
    
    args = parser.parse_args()
    
    # Load available models
    models = load_available_models()
    
    if not models:
        return
    
    print("ğŸš€ Arabic Qwen Model Inference")
    print("==============================")
    
    # Display available models
    display_models(models)
    
    # Benchmark mode
    if args.benchmark:
        if not args.prompt:
            args.prompt = "Ø§ÙƒØªØ¨ ÙÙ‚Ø±Ø© Ù‚ØµÙŠØ±Ø© Ø¹Ù† Ø£Ù‡Ù…ÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ…"
        benchmark_mode(models, args.prompt)
        return
    
    # Select model
    if args.model:
        if args.model not in models:
            print(f"âŒ Model {args.model} not found. Available models: {list(models.keys())}")
            return
        selected_model = models[args.model]
    else:
        # Interactive model selection
        while True:
            try:
                choice = input("\nğŸ”¢ Select a model (number): ").strip()
                if choice in models:
                    selected_model = models[choice]
                    break
                else:
                    print(f"âŒ Invalid choice. Please select from: {list(models.keys())}")
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Goodbye!")
                return
    
    print(f"\nâœ… Selected: {selected_model['name']}")
    
    # Interactive mode
    if args.interactive or not args.prompt:
        interactive_mode(selected_model)
    else:
        # Single inference
        single_inference(selected_model, args.prompt, args.task)
    
    print("\nğŸ“š Note: This is a demonstration script with mock responses.")
    print("For real inference, install transformers and torch, then update the inference functions.")

if __name__ == "__main__":
    main()