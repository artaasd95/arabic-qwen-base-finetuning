#!/usr/bin/env python3
"""
Arabic Qwen Model Usage Examples
Demonstrates how to use the fine-tuned Arabic Qwen models for various tasks
"""

import json
import torch
from pathlib import Path
from typing import List, Dict, Optional

# Mock imports for demonstration (replace with actual imports when packages are available)
class MockTokenizer:
    def __init__(self, model_path):
        self.model_path = model_path
        self.pad_token = "<|endoftext|>"
        self.eos_token = "<|endoftext|>"
    
    def encode(self, text, return_tensors="pt", max_length=512, truncation=True, padding=True):
        # Mock encoding - returns dummy tensor
        return {"input_ids": torch.randint(0, 1000, (1, min(len(text.split()), max_length)))}
    
    def decode(self, token_ids, skip_special_tokens=True):
        # Mock decoding
        return "Ù‡Ø°Ø§ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙÙˆÙ„Ø¯ Ø¨Ø§Ù„Ù„ØºØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."

class MockModel:
    def __init__(self, model_path):
        self.model_path = model_path
        self.device = "cpu"
    
    def generate(self, input_ids, max_length=256, temperature=0.7, do_sample=True, pad_token_id=None):
        # Mock generation - returns dummy tensor
        batch_size, seq_len = input_ids.shape
        new_length = min(max_length, seq_len + 50)
        return torch.randint(0, 1000, (batch_size, new_length))
    
    def to(self, device):
        self.device = device
        return self
    
    def eval(self):
        return self

class ArabicQwenInference:
    """Arabic Qwen Model Inference Class"""
    
    def __init__(self, model_path: str, device: str = "auto"):
        """
        Initialize the Arabic Qwen model for inference
        
        Args:
            model_path: Path to the trained model
            device: Device to run inference on ('cpu', 'cuda', or 'auto')
        """
        self.model_path = Path(model_path)
        self.device = self._get_device(device)
        
        # Load model and tokenizer
        self.tokenizer = self._load_tokenizer()
        self.model = self._load_model()
        
        print(f"âœ“ Loaded model from {model_path} on {self.device}")
    
    def _get_device(self, device: str) -> str:
        """Determine the best device for inference"""
        if device == "auto":
            return "cuda" if torch.cuda.is_available() else "cpu"
        return device
    
    def _load_tokenizer(self):
        """Load the tokenizer"""
        # In real implementation, use:
        # from transformers import AutoTokenizer
        # return AutoTokenizer.from_pretrained(self.model_path)
        return MockTokenizer(self.model_path)
    
    def _load_model(self):
        """Load the model"""
        # In real implementation, use:
        # from transformers import AutoModelForCausalLM
        # model = AutoModelForCausalLM.from_pretrained(self.model_path)
        # return model.to(self.device).eval()
        return MockModel(self.model_path).to(self.device).eval()
    
    def generate_text(self, 
                     prompt: str, 
                     max_length: int = 256, 
                     temperature: float = 0.7, 
                     do_sample: bool = True,
                     num_return_sequences: int = 1) -> List[str]:
        """
        Generate text from a prompt
        
        Args:
            prompt: Input text prompt
            max_length: Maximum length of generated text
            temperature: Sampling temperature (0.0 = deterministic)
            do_sample: Whether to use sampling
            num_return_sequences: Number of sequences to generate
            
        Returns:
            List of generated text sequences
        """
        # Tokenize input
        inputs = self.tokenizer.encode(
            prompt, 
            return_tensors="pt", 
            max_length=512, 
            truncation=True
        )
        
        # Generate
        with torch.no_grad():
            outputs = self.model.generate(
                inputs["input_ids"],
                max_length=max_length,
                temperature=temperature,
                do_sample=do_sample,
                pad_token_id=self.tokenizer.pad_token.encode() if hasattr(self.tokenizer.pad_token, 'encode') else 0,
                num_return_sequences=num_return_sequences
            )
        
        # Decode outputs
        generated_texts = []
        for output in outputs:
            text = self.tokenizer.decode(output, skip_special_tokens=True)
            # Remove the original prompt from the generated text
            if text.startswith(prompt):
                text = text[len(prompt):].strip()
            generated_texts.append(text)
        
        return generated_texts
    
    def chat_completion(self, messages: List[Dict[str, str]]) -> str:
        """
        Generate chat completion
        
        Args:
            messages: List of message dictionaries with 'role' and 'content'
            
        Returns:
            Generated response
        """
        # Format messages into a prompt
        prompt = self._format_chat_prompt(messages)
        
        # Generate response
        responses = self.generate_text(
            prompt, 
            max_length=512, 
            temperature=0.7, 
            do_sample=True
        )
        
        return responses[0] if responses else ""
    
    def _format_chat_prompt(self, messages: List[Dict[str, str]]) -> str:
        """
        Format chat messages into a prompt
        
        Args:
            messages: List of message dictionaries
            
        Returns:
            Formatted prompt string
        """
        prompt_parts = []
        
        for message in messages:
            role = message.get('role', 'user')
            content = message.get('content', '')
            
            if role == 'system':
                prompt_parts.append(f"Ø§Ù„Ù†Ø¸Ø§Ù…: {content}")
            elif role == 'user':
                prompt_parts.append(f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {content}")
            elif role == 'assistant':
                prompt_parts.append(f"Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {content}")
        
        prompt_parts.append("Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯:")
        return "\n".join(prompt_parts)
    
    def question_answering(self, question: str, context: str = "") -> str:
        """
        Answer a question based on optional context
        
        Args:
            question: The question to answer
            context: Optional context for the question
            
        Returns:
            Generated answer
        """
        if context:
            prompt = f"Ø§Ù„Ø³ÙŠØ§Ù‚: {context}\n\nØ§Ù„Ø³Ø¤Ø§Ù„: {question}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
        else:
            prompt = f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}\n\nØ§Ù„Ø¥Ø¬Ø§Ø¨Ø©:"
        
        responses = self.generate_text(
            prompt, 
            max_length=256, 
            temperature=0.3,  # Lower temperature for more focused answers
            do_sample=True
        )
        
        return responses[0] if responses else ""
    
    def instruction_following(self, instruction: str, input_text: str = "") -> str:
        """
        Follow an instruction with optional input
        
        Args:
            instruction: The instruction to follow
            input_text: Optional input text
            
        Returns:
            Generated response
        """
        if input_text:
            prompt = f"### Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n{instruction}\n\n### Ø§Ù„Ù…Ø¯Ø®Ù„:\n{input_text}\n\n### Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:\n"
        else:
            prompt = f"### Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª:\n{instruction}\n\n### Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©:\n"
        
        responses = self.generate_text(
            prompt, 
            max_length=512, 
            temperature=0.7, 
            do_sample=True
        )
        
        return responses[0] if responses else ""

def load_available_models() -> Dict[str, str]:
    """Load available trained models"""
    model_paths_file = Path("outputs/model_paths.json")
    
    if not model_paths_file.exists():
        print("âš ï¸  No trained models found. Please run training first.")
        return {}
    
    with open(model_paths_file, 'r', encoding='utf-8') as f:
        model_paths = json.load(f)
    
    available_models = {}
    for model_info in model_paths:
        model_name = model_info['model_name']
        model_path = model_info['path']
        available_models[model_name] = model_path
    
    return available_models

def demonstrate_sft_model():
    """Demonstrate SFT model usage"""
    print("\n=== SFT Model Demonstration ===")
    print("SFT models are best for general instruction following and text generation.")
    
    # Load SFT model (using first available SFT model)
    available_models = load_available_models()
    sft_models = {k: v for k, v in available_models.items() if 'SFT' in k}
    
    if not sft_models:
        print("âŒ No SFT models found.")
        return
    
    model_name = list(sft_models.keys())[0]
    model_path = sft_models[model_name]
    
    print(f"Loading model: {model_name}")
    model = ArabicQwenInference(model_path)
    
    # Example 1: Instruction following
    print("\n--- Example 1: Instruction Following ---")
    instruction = "Ø§ÙƒØªØ¨ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© Ø¹Ù† Ø·ÙÙ„ ÙŠØ­Ø¨ Ø§Ù„Ù‚Ø±Ø§Ø¡Ø©"
    response = model.instruction_following(instruction)
    print(f"Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: {instruction}")
    print(f"Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response}")
    
    # Example 2: Text completion
    print("\n--- Example 2: Text Completion ---")
    prompt = "Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ù‡Ùˆ"
    response = model.generate_text(prompt, max_length=200, temperature=0.7)
    print(f"Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©: {prompt}")
    print(f"Ø§Ù„Ù†Øµ Ø§Ù„Ù…ÙƒØªÙ…Ù„: {prompt} {response[0]}")

def demonstrate_dpo_model():
    """Demonstrate DPO model usage"""
    print("\n=== DPO Model Demonstration ===")
    print("DPO models are optimized for preference-aligned responses.")
    
    available_models = load_available_models()
    dpo_models = {k: v for k, v in available_models.items() if 'DPO' in k}
    
    if not dpo_models:
        print("âŒ No DPO models found.")
        return
    
    model_name = list(dpo_models.keys())[0]
    model_path = dpo_models[model_name]
    
    print(f"Loading model: {model_name}")
    model = ArabicQwenInference(model_path)
    
    # Example: Chat completion
    print("\n--- Example: Chat Completion ---")
    messages = [
        {"role": "system", "content": "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ ÙˆÙ…ÙÙŠØ¯ ÙŠØªØ­Ø¯Ø« Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."},
        {"role": "user", "content": "Ù…Ø§ Ù‡ÙŠ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¹Ù„Ù… Ù„ØºØ© Ø¬Ø¯ÙŠØ¯Ø©ØŸ"}
    ]
    
    response = model.chat_completion(messages)
    print(f"Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…: {messages[1]['content']}")
    print(f"Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯: {response}")

def demonstrate_kto_model():
    """Demonstrate KTO model usage"""
    print("\n=== KTO Model Demonstration ===")
    print("KTO models use Kahneman-Tversky optimization for better preference learning.")
    
    available_models = load_available_models()
    kto_models = {k: v for k, v in available_models.items() if 'KTO' in k}
    
    if not kto_models:
        print("âŒ No KTO models found.")
        return
    
    model_name = list(kto_models.keys())[0]
    model_path = kto_models[model_name]
    
    print(f"Loading model: {model_name}")
    model = ArabicQwenInference(model_path)
    
    # Example: Question answering
    print("\n--- Example: Question Answering ---")
    question = "Ù…Ø§ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©ØŸ"
    answer = model.question_answering(question)
    print(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}")
    print(f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {answer}")

def demonstrate_ipo_model():
    """Demonstrate IPO model usage"""
    print("\n=== IPO Model Demonstration ===")
    print("IPO models use Identity Preference Optimization for efficient training.")
    
    available_models = load_available_models()
    ipo_models = {k: v for k, v in available_models.items() if 'IPO' in k}
    
    if not ipo_models:
        print("âŒ No IPO models found.")
        return
    
    model_name = list(ipo_models.keys())[0]
    model_path = ipo_models[model_name]
    
    print(f"Loading model: {model_name}")
    model = ArabicQwenInference(model_path)
    
    # Example: Context-based QA
    print("\n--- Example: Context-based Question Answering ---")
    context = "Ø§Ù„Ø±ÙŠØ§Ø¶ Ù‡ÙŠ Ø¹Ø§ØµÙ…Ø© Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© ÙˆØ£ÙƒØ¨Ø± Ù…Ø¯Ù†Ù‡Ø§. ØªÙ‚Ø¹ ÙÙŠ ÙˆØ³Ø· Ø´Ø¨Ù‡ Ø§Ù„Ø¬Ø²ÙŠØ±Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©."
    question = "Ø£ÙŠÙ† ØªÙ‚Ø¹ Ø§Ù„Ø±ÙŠØ§Ø¶ØŸ"
    answer = model.question_answering(question, context)
    print(f"Ø§Ù„Ø³ÙŠØ§Ù‚: {context}")
    print(f"Ø§Ù„Ø³Ø¤Ø§Ù„: {question}")
    print(f"Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©: {answer}")

def demonstrate_cpo_model():
    """Demonstrate CPO model usage"""
    print("\n=== CPO Model Demonstration ===")
    print("CPO models use Contrastive Preference Optimization for better understanding.")
    
    available_models = load_available_models()
    cpo_models = {k: v for k, v in available_models.items() if 'CPO' in k}
    
    if not cpo_models:
        print("âŒ No CPO models found.")
        return
    
    model_name = list(cpo_models.keys())[0]
    model_path = cpo_models[model_name]
    
    print(f"Loading model: {model_name}")
    model = ArabicQwenInference(model_path)
    
    # Example: Complex instruction
    print("\n--- Example: Complex Instruction Following ---")
    instruction = "Ù‚Ø§Ø±Ù† Ø¨ÙŠÙ† Ø§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ø´Ù…Ø³ÙŠØ© ÙˆØ§Ù„Ø·Ø§Ù‚Ø© Ø§Ù„Ù†ÙˆÙˆÙŠØ© Ù…Ù† Ù†Ø§Ø­ÙŠØ© Ø§Ù„ØªÙƒÙ„ÙØ© ÙˆØ§Ù„Ø£Ø«Ø± Ø§Ù„Ø¨ÙŠØ¦ÙŠ"
    response = model.instruction_following(instruction)
    print(f"Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª: {instruction}")
    print(f"Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø©: {response}")

def main():
    """Main demonstration function"""
    print("ğŸš€ Arabic Qwen Model Usage Examples")
    print("====================================")
    
    # Check available models
    available_models = load_available_models()
    
    if not available_models:
        print("âŒ No trained models found. Please run training first.")
        return
    
    print(f"\nğŸ“Š Found {len(available_models)} trained models:")
    for model_name in available_models.keys():
        print(f"  - {model_name}")
    
    # Demonstrate each model type
    try:
        demonstrate_sft_model()
        demonstrate_dpo_model()
        demonstrate_kto_model()
        demonstrate_ipo_model()
        demonstrate_cpo_model()
        
        print("\nâœ… All demonstrations completed successfully!")
        
    except Exception as e:
        print(f"\nâŒ Error during demonstration: {e}")
        print("Note: This is a mock demonstration. Install transformers and torch for real usage.")
    
    # Usage instructions
    print("\nğŸ“š Usage Instructions:")
    print("1. Install required packages: pip install transformers torch")
    print("2. Replace MockTokenizer and MockModel with real implementations")
    print("3. Use the ArabicQwenInference class for your applications")
    print("4. Adjust generation parameters based on your needs")

if __name__ == "__main__":
    main()