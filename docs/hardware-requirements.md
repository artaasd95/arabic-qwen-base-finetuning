# Hardware Requirements for Arabic Qwen Fine-tuning

This guide provides comprehensive hardware requirements and optimization strategies for fine-tuning Arabic Qwen models, with specific focus on RTX 3060 12GB and similar consumer hardware.

## ðŸ“‹ Table of Contents

1. [Hardware Overview](#hardware-overview)
2. [GPU Requirements](#gpu-requirements)
3. [Memory Optimization](#memory-optimization)
4. [Storage Requirements](#storage-requirements)
5. [CPU and System Requirements](#cpu-and-system-requirements)
6. [Performance Benchmarks](#performance-benchmarks)
7. [Cost-Effective Setups](#cost-effective-setups)
8. [Cloud Alternatives](#cloud-alternatives)

## ðŸ–¥ï¸ Hardware Overview

### Minimum vs Recommended Specifications

| Component | Minimum | Recommended | Optimal |
|-----------|---------|-------------|----------|
| **GPU** | RTX 3060 12GB | RTX 4070 Ti 12GB | RTX 4090 24GB |
| **VRAM** | 12GB | 16GB | 24GB+ |
| **System RAM** | 16GB | 32GB | 64GB |
| **Storage** | 500GB SSD | 1TB NVMe SSD | 2TB NVMe SSD |
| **CPU** | 6-core | 8-core | 12+ core |
| **PSU** | 650W | 750W | 850W+ |

### Hardware Compatibility Matrix

| Model Size | RTX 3060 12GB | RTX 4070 12GB | RTX 4070 Ti 12GB | RTX 4080 16GB | RTX 4090 24GB |
|------------|---------------|---------------|------------------|---------------|---------------|
| **Qwen2.5-3B** | âœ… Full | âœ… Full | âœ… Full | âœ… Full | âœ… Full |
| **Qwen3-1.7B** | âœ… Full | âœ… Full | âœ… Full | âœ… Full | âœ… Full |
| **Qwen2.5-7B** | âš ï¸ QLoRA Only | âœ… QLoRA/LoRA | âœ… Full | âœ… Full | âœ… Full |
| **Qwen2.5-14B** | âŒ Not Feasible | âš ï¸ QLoRA Only | âš ï¸ QLoRA Only | âœ… QLoRA/LoRA | âœ… Full |
| **Qwen2.5-32B** | âŒ Not Feasible | âŒ Not Feasible | âŒ Not Feasible | âš ï¸ QLoRA Only | âœ… QLoRA/LoRA |

**Legend:**
- âœ… **Full**: Full fine-tuning supported
- âœ… **QLoRA/LoRA**: Parameter-efficient fine-tuning
- âš ï¸ **QLoRA Only**: Only QLoRA with heavy optimization
- âŒ **Not Feasible**: Cannot fit in memory

## ðŸŽ® GPU Requirements

### RTX 3060 12GB Optimization

#### Memory Usage Breakdown

```python
# Memory usage estimation for RTX 3060 12GB
class RTX3060MemoryCalculator:
    def __init__(self):
        self.total_vram = 12 * 1024  # 12GB in MB
        self.system_overhead = 1024  # 1GB for system
        self.available_vram = self.total_vram - self.system_overhead
        
    def calculate_model_memory(self, model_params_b: float, precision: str = "fp16") -> dict:
        """Calculate memory requirements for model"""
        
        precision_multipliers = {
            "fp32": 4,  # 4 bytes per parameter
            "fp16": 2,  # 2 bytes per parameter
            "int8": 1,  # 1 byte per parameter
            "int4": 0.5 # 0.5 bytes per parameter
        }
        
        bytes_per_param = precision_multipliers[precision]
        model_size_mb = model_params_b * 1000 * bytes_per_param
        
        # Additional memory for gradients, optimizer states, activations
        gradient_memory = model_size_mb  # Same as model for gradients
        optimizer_memory = model_size_mb * 2  # Adam optimizer states
        activation_memory = model_size_mb * 0.5  # Activation memory
        
        total_memory = model_size_mb + gradient_memory + optimizer_memory + activation_memory
        
        return {
            "model_size_mb": model_size_mb,
            "gradient_memory_mb": gradient_memory,
            "optimizer_memory_mb": optimizer_memory,
            "activation_memory_mb": activation_memory,
            "total_memory_mb": total_memory,
            "fits_in_vram": total_memory <= self.available_vram,
            "memory_utilization": (total_memory / self.available_vram) * 100
        }
    
    def recommend_optimization(self, model_params_b: float) -> dict:
        """Recommend optimization strategy"""
        
        strategies = []
        
        # Check different precision levels
        fp16_calc = self.calculate_model_memory(model_params_b, "fp16")
        int8_calc = self.calculate_model_memory(model_params_b, "int8")
        int4_calc = self.calculate_model_memory(model_params_b, "int4")
        
        if fp16_calc["fits_in_vram"]:
            strategies.append({
                "method": "Full Fine-tuning (FP16)",
                "memory_usage": fp16_calc["total_memory_mb"],
                "utilization": fp16_calc["memory_utilization"],
                "feasible": True
            })
        
        if int8_calc["fits_in_vram"]:
            strategies.append({
                "method": "8-bit Fine-tuning",
                "memory_usage": int8_calc["total_memory_mb"],
                "utilization": int8_calc["memory_utilization"],
                "feasible": True
            })
        
        # QLoRA estimation (much lower memory)
        qlora_memory = model_params_b * 1000 * 0.5 + 2048  # Rough estimate
        strategies.append({
            "method": "QLoRA (4-bit + LoRA)",
            "memory_usage": qlora_memory,
            "utilization": (qlora_memory / self.available_vram) * 100,
            "feasible": qlora_memory <= self.available_vram
        })
        
        return {
            "model_params": model_params_b,
            "available_vram_mb": self.available_vram,
            "strategies": strategies
        }

# Usage examples
calculator = RTX3060MemoryCalculator()

# Check different models
models = {
    "Qwen2.5-3B": 3.0,
    "Qwen2.5-7B": 7.0,
    "Qwen2.5-14B": 14.0
}

for model_name, params in models.items():
    print(f"\n=== {model_name} ===")
    recommendations = calculator.recommend_optimization(params)
    
    for strategy in recommendations["strategies"]:
        if strategy["feasible"]:
            print(f"âœ… {strategy['method']}: {strategy['memory_usage']:.0f}MB ({strategy['utilization']:.1f}% VRAM)")
        else:
            print(f"âŒ {strategy['method']}: {strategy['memory_usage']:.0f}MB (exceeds VRAM)")
```

#### RTX 3060 Optimization Settings

```python
# Optimal training configuration for RTX 3060 12GB
RTX_3060_CONFIGS = {
    "qwen2.5-3b": {
        "method": "full_finetuning",
        "precision": "fp16",
        "batch_size": 4,
        "gradient_accumulation_steps": 4,
        "max_seq_length": 2048,
        "learning_rate": 2e-5,
        "memory_optimization": {
            "gradient_checkpointing": True,
            "dataloader_pin_memory": False,
            "empty_cache_steps": 100
        }
    },
    "qwen2.5-7b": {
        "method": "qlora",
        "precision": "4bit",
        "batch_size": 2,
        "gradient_accumulation_steps": 8,
        "max_seq_length": 1024,
        "learning_rate": 1e-4,
        "lora_config": {
            "r": 16,
            "lora_alpha": 32,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
            "lora_dropout": 0.1
        },
        "memory_optimization": {
            "gradient_checkpointing": True,
            "dataloader_pin_memory": False,
            "empty_cache_steps": 50,
            "cpu_offload": True
        }
    }
}
```

### GPU Performance Comparison

| GPU Model | VRAM | Memory Bandwidth | CUDA Cores | Tensor Cores | Price Range | Performance Score |
|-----------|------|------------------|------------|--------------|-------------|-------------------|
| **RTX 3060 12GB** | 12GB | 360 GB/s | 3584 | 112 (2nd gen) | $300-400 | 7/10 |
| **RTX 4060 Ti 16GB** | 16GB | 288 GB/s | 4352 | 136 (3rd gen) | $500-600 | 8/10 |
| **RTX 4070 12GB** | 12GB | 504 GB/s | 5888 | 184 (3rd gen) | $600-700 | 8.5/10 |
| **RTX 4070 Ti 12GB** | 12GB | 504 GB/s | 7680 | 240 (3rd gen) | $700-800 | 9/10 |
| **RTX 4080 16GB** | 16GB | 717 GB/s | 9728 | 304 (3rd gen) | $1000-1200 | 9.5/10 |
| **RTX 4090 24GB** | 24GB | 1008 GB/s | 16384 | 512 (3rd gen) | $1500-2000 | 10/10 |

## ðŸ’¾ Memory Optimization

### System RAM Requirements

#### Memory Usage Patterns

```python
class SystemMemoryCalculator:
    def __init__(self):
        self.os_overhead = 4 * 1024  # 4GB for OS
        self.browser_overhead = 2 * 1024  # 2GB for browser/other apps
        
    def calculate_training_memory(self, dataset_size_gb: float, model_size_gb: float) -> dict:
        """Calculate system memory requirements"""
        
        # Dataset loading (depends on caching strategy)
        dataset_memory = min(dataset_size_gb * 1024, 8 * 1024)  # Max 8GB cache
        
        # Model loading (CPU copy for some operations)
        model_memory = model_size_gb * 1024 * 0.5  # Partial CPU copy
        
        # Training overhead (tokenization, data processing)
        training_overhead = 4 * 1024  # 4GB overhead
        
        # Total system memory needed
        total_needed = (
            self.os_overhead + 
            self.browser_overhead + 
            dataset_memory + 
            model_memory + 
            training_overhead
        )
        
        return {
            "os_overhead_mb": self.os_overhead,
            "dataset_cache_mb": dataset_memory,
            "model_memory_mb": model_memory,
            "training_overhead_mb": training_overhead,
            "total_needed_mb": total_needed,
            "recommended_ram_gb": int((total_needed / 1024) * 1.2)  # 20% buffer
        }
    
    def recommend_ram_config(self, use_case: str) -> dict:
        """Recommend RAM configuration"""
        
        configs = {
            "basic_3b": {
                "dataset_size_gb": 2,
                "model_size_gb": 6,
                "description": "Basic 3B model fine-tuning"
            },
            "advanced_7b": {
                "dataset_size_gb": 5,
                "model_size_gb": 14,
                "description": "Advanced 7B model with large dataset"
            },
            "production_14b": {
                "dataset_size_gb": 10,
                "model_size_gb": 28,
                "description": "Production 14B model training"
            }
        }
        
        if use_case not in configs:
            use_case = "basic_3b"
            
        config = configs[use_case]
        memory_calc = self.calculate_training_memory(
            config["dataset_size_gb"], 
            config["model_size_gb"]
        )
        
        return {
            "use_case": config["description"],
            "memory_breakdown": memory_calc,
            "recommended_configs": [
                {
                    "ram_size": "16GB",
                    "suitable": memory_calc["recommended_ram_gb"] <= 16,
                    "performance": "Basic" if memory_calc["recommended_ram_gb"] <= 16 else "Insufficient"
                },
                {
                    "ram_size": "32GB",
                    "suitable": True,
                    "performance": "Optimal"
                },
                {
                    "ram_size": "64GB",
                    "suitable": True,
                    "performance": "Overkill (unless multiple models)"
                }
            ]
        }

# Usage
mem_calc = SystemMemoryCalculator()

for use_case in ["basic_3b", "advanced_7b", "production_14b"]:
    print(f"\n=== {use_case.upper()} ===")
    recommendation = mem_calc.recommend_ram_config(use_case)
    print(f"Use case: {recommendation['use_case']}")
    print(f"Recommended RAM: {recommendation['memory_breakdown']['recommended_ram_gb']}GB")
    
    for config in recommendation["recommended_configs"]:
        status = "âœ…" if config["suitable"] else "âŒ"
        print(f"{status} {config['ram_size']}: {config['performance']}")
```

### Memory Optimization Techniques

#### 1. Gradient Checkpointing

```python
# Enable gradient checkpointing to trade compute for memory
training_args = TrainingArguments(
    gradient_checkpointing=True,  # Reduces memory by ~30-50%
    dataloader_pin_memory=False,  # Reduces system RAM usage
    remove_unused_columns=False,  # Keep all columns for debugging
    fp16=True,  # Use half precision
    tf32=True,  # Use TensorFloat-32 on Ampere GPUs
)
```

#### 2. Dynamic Batch Sizing

```python
class DynamicBatchSizer:
    def __init__(self, initial_batch_size: int = 4):
        self.current_batch_size = initial_batch_size
        self.min_batch_size = 1
        self.max_batch_size = 16
        
    def adjust_batch_size(self, memory_error: bool = False, underutilized: bool = False):
        """Dynamically adjust batch size based on memory usage"""
        if memory_error and self.current_batch_size > self.min_batch_size:
            self.current_batch_size = max(self.min_batch_size, self.current_batch_size // 2)
            print(f"Reduced batch size to {self.current_batch_size} due to OOM")
            
        elif underutilized and self.current_batch_size < self.max_batch_size:
            self.current_batch_size = min(self.max_batch_size, self.current_batch_size * 2)
            print(f"Increased batch size to {self.current_batch_size}")
            
        return self.current_batch_size

# Usage in training loop
batch_sizer = DynamicBatchSizer(initial_batch_size=4)

try:
    # Training code here
    pass
except torch.cuda.OutOfMemoryError:
    torch.cuda.empty_cache()
    new_batch_size = batch_sizer.adjust_batch_size(memory_error=True)
    # Restart training with new batch size
```

#### 3. Memory Monitoring

```python
import torch
import psutil
import GPUtil

class MemoryMonitor:
    def __init__(self):
        self.gpu_id = 0
        
    def get_gpu_memory_info(self) -> dict:
        """Get current GPU memory usage"""
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.memory_stats(self.gpu_id)
            allocated = gpu_memory['allocated_bytes.all.current'] / 1024**3
            reserved = gpu_memory['reserved_bytes.all.current'] / 1024**3
            max_allocated = gpu_memory['allocated_bytes.all.peak'] / 1024**3
            
            return {
                "allocated_gb": allocated,
                "reserved_gb": reserved,
                "max_allocated_gb": max_allocated,
                "free_gb": torch.cuda.get_device_properties(self.gpu_id).total_memory / 1024**3 - reserved
            }
        return {}
    
    def get_system_memory_info(self) -> dict:
        """Get current system memory usage"""
        memory = psutil.virtual_memory()
        return {
            "total_gb": memory.total / 1024**3,
            "available_gb": memory.available / 1024**3,
            "used_gb": memory.used / 1024**3,
            "percentage": memory.percent
        }
    
    def print_memory_status(self):
        """Print comprehensive memory status"""
        gpu_info = self.get_gpu_memory_info()
        sys_info = self.get_system_memory_info()
        
        print("\n=== Memory Status ===")
        if gpu_info:
            print(f"GPU Memory:")
            print(f"  Allocated: {gpu_info['allocated_gb']:.2f}GB")
            print(f"  Reserved: {gpu_info['reserved_gb']:.2f}GB")
            print(f"  Free: {gpu_info['free_gb']:.2f}GB")
            print(f"  Peak: {gpu_info['max_allocated_gb']:.2f}GB")
            
        print(f"System Memory:")
        print(f"  Used: {sys_info['used_gb']:.2f}GB / {sys_info['total_gb']:.2f}GB ({sys_info['percentage']:.1f}%)")
        print(f"  Available: {sys_info['available_gb']:.2f}GB")

# Usage
monitor = MemoryMonitor()
monitor.print_memory_status()

# Use during training
def training_step_with_monitoring(model, batch):
    # Training step
    outputs = model(**batch)
    loss = outputs.loss
    
    # Monitor memory every 100 steps
    if step % 100 == 0:
        monitor.print_memory_status()
        
    return loss
```

## ðŸ’¿ Storage Requirements

### Storage Capacity Planning

| Component | Size Range | Description | Recommended Type |
|-----------|------------|-------------|------------------|
| **Base Models** | 2-50GB | Original model weights | NVMe SSD |
| **Datasets** | 1-100GB | Training data | SATA SSD |
| **Checkpoints** | 2-50GB each | Training snapshots | NVMe SSD |
| **Logs & Metrics** | 100MB-1GB | Training logs | Any SSD |
| **Fine-tuned Models** | 2-50GB each | Final outputs | NVMe SSD |
| **Cache & Temp** | 10-50GB | Temporary files | NVMe SSD |

### Storage Configuration Examples

#### Budget Setup (500GB Total)
```
ðŸ“ Storage Layout - Budget (500GB NVMe SSD)
â”œâ”€â”€ ðŸ“‚ models/           (200GB)
â”‚   â”œâ”€â”€ base_models/     (100GB) - 2-3 base models
â”‚   â””â”€â”€ fine_tuned/      (100GB) - 2-3 fine-tuned models
â”œâ”€â”€ ðŸ“‚ datasets/         (150GB)
â”‚   â”œâ”€â”€ raw/             (50GB)  - Original datasets
â”‚   â”œâ”€â”€ processed/       (50GB)  - Preprocessed data
â”‚   â””â”€â”€ cache/           (50GB)  - HuggingFace cache
â”œâ”€â”€ ðŸ“‚ training/         (100GB)
â”‚   â”œâ”€â”€ checkpoints/     (80GB)  - Training checkpoints
â”‚   â””â”€â”€ logs/            (20GB)  - Training logs
â””â”€â”€ ðŸ“‚ temp/             (50GB)   - Temporary files
```

#### Recommended Setup (1TB Total)
```
ðŸ“ Storage Layout - Recommended (1TB NVMe SSD)
â”œâ”€â”€ ðŸ“‚ models/           (400GB)
â”‚   â”œâ”€â”€ base_models/     (200GB) - 5-6 base models
â”‚   â””â”€â”€ fine_tuned/      (200GB) - 5-6 fine-tuned models
â”œâ”€â”€ ðŸ“‚ datasets/         (300GB)
â”‚   â”œâ”€â”€ raw/             (100GB) - Multiple datasets
â”‚   â”œâ”€â”€ processed/       (100GB) - Preprocessed variants
â”‚   â””â”€â”€ cache/           (100GB) - Extended cache
â”œâ”€â”€ ðŸ“‚ training/         (200GB)
â”‚   â”œâ”€â”€ checkpoints/     (150GB) - Multiple experiments
â”‚   â””â”€â”€ logs/            (50GB)  - Detailed logs
â””â”€â”€ ðŸ“‚ temp/             (100GB)  - Large temporary files
```

#### Professional Setup (2TB Total)
```
ðŸ“ Storage Layout - Professional (2TB NVMe SSD)
â”œâ”€â”€ ðŸ“‚ models/           (800GB)
â”‚   â”œâ”€â”€ base_models/     (300GB) - Complete model collection
â”‚   â”œâ”€â”€ fine_tuned/      (300GB) - Multiple variants
â”‚   â””â”€â”€ experiments/     (200GB) - Experimental models
â”œâ”€â”€ ðŸ“‚ datasets/         (600GB)
â”‚   â”œâ”€â”€ raw/             (200GB) - Large dataset collection
â”‚   â”œâ”€â”€ processed/       (200GB) - Multiple preprocessing
â”‚   â”œâ”€â”€ augmented/       (100GB) - Data augmentation
â”‚   â””â”€â”€ cache/           (100GB) - Full cache
â”œâ”€â”€ ðŸ“‚ training/         (400GB)
â”‚   â”œâ”€â”€ checkpoints/     (300GB) - Comprehensive experiments
â”‚   â””â”€â”€ logs/            (100GB) - Detailed analytics
â””â”€â”€ ðŸ“‚ backup/           (200GB)  - Local backups
```

### Storage Performance Optimization

```python
import os
import time
from pathlib import Path

class StorageOptimizer:
    def __init__(self, base_path: str):
        self.base_path = Path(base_path)
        self.setup_directory_structure()
        
    def setup_directory_structure(self):
        """Create optimized directory structure"""
        directories = [
            "models/base_models",
            "models/fine_tuned",
            "models/checkpoints",
            "datasets/raw",
            "datasets/processed",
            "datasets/cache",
            "training/logs",
            "training/tensorboard",
            "temp"
        ]
        
        for directory in directories:
            (self.base_path / directory).mkdir(parents=True, exist_ok=True)
            
    def benchmark_storage_speed(self, test_size_mb: int = 1000) -> dict:
        """Benchmark storage read/write speeds"""
        test_file = self.base_path / "temp" / "speed_test.bin"
        test_data = os.urandom(test_size_mb * 1024 * 1024)
        
        # Write speed test
        start_time = time.time()
        with open(test_file, "wb") as f:
            f.write(test_data)
        write_time = time.time() - start_time
        write_speed = test_size_mb / write_time
        
        # Read speed test
        start_time = time.time()
        with open(test_file, "rb") as f:
            _ = f.read()
        read_time = time.time() - start_time
        read_speed = test_size_mb / read_time
        
        # Cleanup
        test_file.unlink()
        
        return {
            "write_speed_mbps": write_speed,
            "read_speed_mbps": read_speed,
            "write_time_seconds": write_time,
            "read_time_seconds": read_time
        }
    
    def optimize_cache_settings(self) -> dict:
        """Optimize HuggingFace cache settings"""
        cache_dir = self.base_path / "datasets" / "cache"
        
        # Set environment variables for optimal caching
        cache_settings = {
            "HF_DATASETS_CACHE": str(cache_dir),
            "TRANSFORMERS_CACHE": str(cache_dir / "transformers"),
            "HF_HOME": str(cache_dir / "huggingface"),
            "TORCH_HOME": str(cache_dir / "torch")
        }
        
        for key, value in cache_settings.items():
            os.environ[key] = value
            Path(value).mkdir(parents=True, exist_ok=True)
            
        return cache_settings
    
    def cleanup_temp_files(self, max_age_hours: int = 24):
        """Clean up temporary files older than specified hours"""
        temp_dir = self.base_path / "temp"
        current_time = time.time()
        max_age_seconds = max_age_hours * 3600
        
        cleaned_files = 0
        freed_space = 0
        
        for file_path in temp_dir.rglob("*"):
            if file_path.is_file():
                file_age = current_time - file_path.stat().st_mtime
                if file_age > max_age_seconds:
                    file_size = file_path.stat().st_size
                    file_path.unlink()
                    cleaned_files += 1
                    freed_space += file_size
                    
        return {
            "cleaned_files": cleaned_files,
            "freed_space_mb": freed_space / (1024 * 1024)
        }

# Usage
optimizer = StorageOptimizer("/path/to/arabic-qwen-training")

# Benchmark storage
speed_test = optimizer.benchmark_storage_speed()
print(f"Storage Speed Test:")
print(f"Write: {speed_test['write_speed_mbps']:.1f} MB/s")
print(f"Read: {speed_test['read_speed_mbps']:.1f} MB/s")

# Optimize cache
cache_settings = optimizer.optimize_cache_settings()
print(f"\nCache directories configured:")
for key, value in cache_settings.items():
    print(f"{key}: {value}")

# Cleanup
cleanup_result = optimizer.cleanup_temp_files()
print(f"\nCleanup: {cleanup_result['cleaned_files']} files, {cleanup_result['freed_space_mb']:.1f} MB freed")
```

## ðŸ–¥ï¸ CPU and System Requirements

### CPU Performance Impact

| CPU Tier | Cores/Threads | Examples | Training Impact | Recommended Use |
|----------|---------------|----------|-----------------|------------------|
| **Budget** | 6C/12T | Ryzen 5 5600X, i5-12400 | Basic preprocessing | Small datasets |
| **Mainstream** | 8C/16T | Ryzen 7 5700X, i7-12700 | Good performance | Most use cases |
| **High-End** | 12C/24T+ | Ryzen 9 5900X, i9-12900K | Optimal performance | Large datasets |
| **Workstation** | 16C/32T+ | Threadripper, Xeon | Maximum throughput | Production |

### CPU Optimization Settings

```python
import torch
import os
from multiprocessing import cpu_count

class CPUOptimizer:
    def __init__(self):
        self.cpu_count = cpu_count()
        self.physical_cores = self.cpu_count // 2  # Assuming hyperthreading
        
    def optimize_torch_settings(self) -> dict:
        """Optimize PyTorch CPU settings"""
        
        # Set optimal thread counts
        torch.set_num_threads(self.physical_cores)
        torch.set_num_interop_threads(2)
        
        # Enable optimizations
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.deterministic = False
        
        settings = {
            "torch_threads": self.physical_cores,
            "interop_threads": 2,
            "cudnn_benchmark": True,
            "total_cpu_cores": self.cpu_count,
            "physical_cores": self.physical_cores
        }
        
        return settings
    
    def optimize_dataloader_workers(self, batch_size: int) -> int:
        """Calculate optimal number of DataLoader workers"""
        
        # Rule of thumb: 2-4 workers per GPU, but not more than physical cores
        optimal_workers = min(
            self.physical_cores - 2,  # Leave 2 cores for main process
            batch_size,  # Don't exceed batch size
            8  # Cap at 8 for diminishing returns
        )
        
        return max(0, optimal_workers)
    
    def set_environment_variables(self):
        """Set optimal environment variables"""
        env_vars = {
            "OMP_NUM_THREADS": str(self.physical_cores),
            "MKL_NUM_THREADS": str(self.physical_cores),
            "NUMEXPR_NUM_THREADS": str(self.physical_cores),
            "TOKENIZERS_PARALLELISM": "true"
        }
        
        for key, value in env_vars.items():
            os.environ[key] = value
            
        return env_vars

# Usage
cpu_optimizer = CPUOptimizer()

# Optimize PyTorch
torch_settings = cpu_optimizer.optimize_torch_settings()
print(f"PyTorch optimized: {torch_settings}")

# Set environment variables
env_vars = cpu_optimizer.set_environment_variables()
print(f"Environment variables set: {env_vars}")

# Calculate optimal workers for DataLoader
optimal_workers = cpu_optimizer.optimize_dataloader_workers(batch_size=4)
print(f"Optimal DataLoader workers: {optimal_workers}")
```

## ðŸ“Š Performance Benchmarks

### Training Speed Benchmarks

#### RTX 3060 12GB Performance

| Model | Method | Batch Size | Seq Length | Tokens/sec | Time/Epoch | Memory Usage |
|-------|--------|------------|------------|------------|------------|-------------|
| **Qwen2.5-3B** | Full FT | 4 | 2048 | 1,200 | 45 min | 10.5GB |
| **Qwen2.5-3B** | LoRA | 8 | 2048 | 2,000 | 28 min | 8.2GB |
| **Qwen2.5-7B** | QLoRA | 2 | 1024 | 800 | 75 min | 11.8GB |
| **Qwen2.5-7B** | QLoRA | 4 | 512 | 1,400 | 42 min | 11.2GB |

#### Comparative Performance (Different GPUs)

| GPU | Model | Method | Relative Speed | Memory Efficiency | Cost/Performance |
|-----|-------|--------|----------------|-------------------|------------------|
| **RTX 3060 12GB** | Qwen2.5-3B | Full FT | 1.0x | Good | Excellent |
| **RTX 4070 12GB** | Qwen2.5-3B | Full FT | 1.4x | Good | Good |
| **RTX 4070 Ti 12GB** | Qwen2.5-7B | Full FT | 2.1x | Excellent | Good |
| **RTX 4080 16GB** | Qwen2.5-7B | Full FT | 2.8x | Excellent | Fair |
| **RTX 4090 24GB** | Qwen2.5-14B | Full FT | 4.2x | Excellent | Poor |

### Benchmark Testing Script

```python
import time
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from datasets import Dataset

class PerformanceBenchmark:
    def __init__(self, model_name: str, device: str = "cuda"):
        self.model_name = model_name
        self.device = device
        self.results = {}
        
    def benchmark_model_loading(self) -> dict:
        """Benchmark model loading time and memory"""
        torch.cuda.empty_cache()
        start_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        start_time = time.time()
        
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        load_time = time.time() - start_time
        end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        model_memory = (end_memory - start_memory) / 1024**3  # GB
        
        return {
            "load_time_seconds": load_time,
            "model_memory_gb": model_memory,
            "model_parameters": sum(p.numel() for p in model.parameters()) / 1e9
        }
    
    def benchmark_inference_speed(self, model, tokenizer, num_samples: int = 100) -> dict:
        """Benchmark inference speed"""
        test_prompts = [
            "Ø§ÙƒØªØ¨ Ù‚ØµØ© Ù‚ØµÙŠØ±Ø© Ø¹Ù†",
            "Ù…Ø§ Ù‡Ùˆ Ø£ÙØ¶Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¹Ù„Ù…",
            "Ø´Ø±Ø­ Ù…ÙÙ‡ÙˆÙ… Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ"
        ] * (num_samples // 3 + 1)
        
        total_tokens = 0
        total_time = 0
        
        model.eval()
        with torch.no_grad():
            for prompt in test_prompts[:num_samples]:
                inputs = tokenizer(prompt, return_tensors="pt").to(self.device)
                
                start_time = time.time()
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=50,
                    do_sample=True,
                    temperature=0.7,
                    pad_token_id=tokenizer.eos_token_id
                )
                inference_time = time.time() - start_time
                
                generated_tokens = outputs.shape[1] - inputs.input_ids.shape[1]
                total_tokens += generated_tokens
                total_time += inference_time
        
        return {
            "avg_tokens_per_second": total_tokens / total_time,
            "avg_time_per_token_ms": (total_time / total_tokens) * 1000,
            "total_samples": num_samples,
            "total_tokens_generated": total_tokens
        }
    
    def benchmark_training_step(self, model, tokenizer, batch_size: int = 4) -> dict:
        """Benchmark single training step"""
        # Create dummy training data
        dummy_texts = ["Ù‡Ø°Ø§ Ù†Øµ ØªØ¬Ø±ÙŠØ¨ÙŠ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ " * 50] * batch_size
        inputs = tokenizer(
            dummy_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(self.device)
        
        model.train()
        optimizer = torch.optim.AdamW(model.parameters(), lr=1e-5)
        
        # Warmup
        for _ in range(3):
            outputs = model(**inputs, labels=inputs.input_ids)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            torch.cuda.empty_cache()
        
        # Actual benchmark
        torch.cuda.synchronize()
        start_time = time.time()
        
        outputs = model(**inputs, labels=inputs.input_ids)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
        
        torch.cuda.synchronize()
        step_time = time.time() - start_time
        
        # Calculate throughput
        total_tokens = inputs.input_ids.numel()
        tokens_per_second = total_tokens / step_time
        
        return {
            "step_time_seconds": step_time,
            "tokens_per_second": tokens_per_second,
            "batch_size": batch_size,
            "sequence_length": inputs.input_ids.shape[1],
            "loss_value": loss.item()
        }
    
    def run_full_benchmark(self) -> dict:
        """Run complete benchmark suite"""
        print(f"Running benchmark for {self.model_name}...")
        
        # Model loading benchmark
        print("Benchmarking model loading...")
        loading_results = self.benchmark_model_loading()
        
        # Load model for other tests
        tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        model = AutoModelForCausalLM.from_pretrained(
            self.model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        
        # Inference benchmark
        print("Benchmarking inference speed...")
        inference_results = self.benchmark_inference_speed(model, tokenizer)
        
        # Training benchmark
        print("Benchmarking training step...")
        training_results = self.benchmark_training_step(model, tokenizer)
        
        return {
            "model_name": self.model_name,
            "device": self.device,
            "loading": loading_results,
            "inference": inference_results,
            "training": training_results,
            "timestamp": time.time()
        }

# Usage
if __name__ == "__main__":
    models_to_test = [
        "Qwen/Qwen2.5-3B",
        "Qwen/Qwen2.5-7B"
    ]
    
    for model_name in models_to_test:
        try:
            benchmark = PerformanceBenchmark(model_name)
            results = benchmark.run_full_benchmark()
            
            print(f"\n=== {model_name} Results ===")
            print(f"Loading time: {results['loading']['load_time_seconds']:.1f}s")
            print(f"Model memory: {results['loading']['model_memory_gb']:.1f}GB")
            print(f"Inference speed: {results['inference']['avg_tokens_per_second']:.1f} tokens/s")
            print(f"Training speed: {results['training']['tokens_per_second']:.1f} tokens/s")
            
        except Exception as e:
            print(f"Benchmark failed for {model_name}: {e}")
```

## ðŸ’° Cost-Effective Setups

### Budget Configurations

#### Entry Level ($800-1200)
```
ðŸ–¥ï¸ Entry Level Arabic Qwen Setup
â”œâ”€â”€ GPU: RTX 3060 12GB ($350)
â”œâ”€â”€ CPU: Ryzen 5 5600X ($150)
â”œâ”€â”€ RAM: 16GB DDR4-3200 ($60)
â”œâ”€â”€ Storage: 500GB NVMe SSD ($50)
â”œâ”€â”€ Motherboard: B450/B550 ($80)
â”œâ”€â”€ PSU: 650W 80+ Gold ($70)
â”œâ”€â”€ Case + Cooling ($100)
â””â”€â”€ Total: ~$860

Capabilities:
âœ… Qwen2.5-3B full fine-tuning
âœ… Qwen2.5-7B QLoRA fine-tuning
âš ï¸ Limited to smaller datasets
âŒ Cannot handle 14B+ models
```

#### Mainstream ($1200-1800)
```
ðŸ–¥ï¸ Mainstream Arabic Qwen Setup
â”œâ”€â”€ GPU: RTX 4070 12GB ($600)
â”œâ”€â”€ CPU: Ryzen 7 5700X ($200)
â”œâ”€â”€ RAM: 32GB DDR4-3200 ($120)
â”œâ”€â”€ Storage: 1TB NVMe SSD ($80)
â”œâ”€â”€ Motherboard: B550/X570 ($120)
â”œâ”€â”€ PSU: 750W 80+ Gold ($90)
â”œâ”€â”€ Case + Cooling ($150)
â””â”€â”€ Total: ~$1,360

Capabilities:
âœ… All models up to 7B full fine-tuning
âœ… Qwen2.5-14B QLoRA fine-tuning
âœ… Large dataset processing
âœ… Multiple concurrent experiments
```

#### High-End ($1800-2500)
```
ðŸ–¥ï¸ High-End Arabic Qwen Setup
â”œâ”€â”€ GPU: RTX 4080 16GB ($1,100)
â”œâ”€â”€ CPU: Ryzen 9 5900X ($300)
â”œâ”€â”€ RAM: 64GB DDR4-3200 ($240)
â”œâ”€â”€ Storage: 2TB NVMe SSD ($160)
â”œâ”€â”€ Motherboard: X570 ($150)
â”œâ”€â”€ PSU: 850W 80+ Gold ($120)
â”œâ”€â”€ Case + Cooling ($200)
â””â”€â”€ Total: ~$2,270

Capabilities:
âœ… All models up to 14B full fine-tuning
âœ… Production-ready performance
âœ… Multiple large datasets
âœ… Advanced experimentation
```

### ROI Analysis

```python
class ROICalculator:
    def __init__(self):
        self.cloud_costs = {
            "aws_p3.2xlarge": 3.06,  # $/hour (V100 16GB)
            "aws_p4d.xlarge": 3.25,  # $/hour (A100 40GB)
            "gcp_v100": 2.48,        # $/hour (V100 16GB)
            "azure_nc6s_v3": 3.06    # $/hour (V100 16GB)
        }
        
    def calculate_local_setup_roi(self, 
                                 hardware_cost: float,
                                 monthly_training_hours: float,
                                 months_of_use: int = 24) -> dict:
        """Calculate ROI for local hardware setup"""
        
        # Estimate cloud costs
        avg_cloud_cost_per_hour = sum(self.cloud_costs.values()) / len(self.cloud_costs)
        total_cloud_cost = avg_cloud_cost_per_hour * monthly_training_hours * months_of_use
        
        # Local costs
        electricity_cost_per_hour = 0.15  # Assume $0.15/hour for 300W system
        total_electricity_cost = electricity_cost_per_hour * monthly_training_hours * months_of_use
        total_local_cost = hardware_cost + total_electricity_cost
        
        # Calculate savings and ROI
        total_savings = total_cloud_cost - total_local_cost
        roi_percentage = (total_savings / hardware_cost) * 100
        breakeven_months = hardware_cost / (avg_cloud_cost_per_hour * monthly_training_hours)
        
        return {
            "hardware_cost": hardware_cost,
            "total_cloud_cost": total_cloud_cost,
            "total_local_cost": total_local_cost,
            "total_savings": total_savings,
            "roi_percentage": roi_percentage,
            "breakeven_months": breakeven_months,
            "monthly_training_hours": monthly_training_hours,
            "analysis_period_months": months_of_use
        }
    
    def compare_setups(self, setups: dict, monthly_hours: float) -> dict:
        """Compare multiple hardware setups"""
        results = {}
        
        for setup_name, cost in setups.items():
            results[setup_name] = self.calculate_local_setup_roi(cost, monthly_hours)
            
        return results

# Usage
roi_calc = ROICalculator()

setups = {
    "Entry Level": 860,
    "Mainstream": 1360,
    "High-End": 2270
}

# Compare for different usage patterns
usage_patterns = {
    "Light Use": 20,      # 20 hours/month
    "Regular Use": 50,    # 50 hours/month
    "Heavy Use": 100,     # 100 hours/month
    "Professional": 200   # 200 hours/month
}

for pattern_name, hours in usage_patterns.items():
    print(f"\n=== {pattern_name} ({hours} hours/month) ===")
    
    comparison = roi_calc.compare_setups(setups, hours)
    
    for setup_name, analysis in comparison.items():
        print(f"\n{setup_name}:")
        print(f"  Breakeven: {analysis['breakeven_months']:.1f} months")
        print(f"  2-year savings: ${analysis['total_savings']:,.0f}")
        print(f"  ROI: {analysis['roi_percentage']:.1f}%")
```

## â˜ï¸ Cloud Alternatives

### Cloud Platform Comparison

| Platform | Instance Type | GPU | VRAM | Cost/Hour | Best For |
|----------|---------------|-----|------|-----------|----------|
| **Google Colab Pro+** | - | A100 | 40GB | $50/month | Experimentation |
| **AWS SageMaker** | ml.p3.2xlarge | V100 | 16GB | $3.06/hour | Production |
| **Google Cloud** | n1-standard-4 + V100 | V100 | 16GB | $2.48/hour | Cost-effective |
| **Azure ML** | Standard_NC6s_v3 | V100 | 16GB | $3.06/hour | Enterprise |
| **Lambda Labs** | gpu_1x_a100_sxm4 | A100 | 40GB | $1.10/hour | Best value |
| **RunPod** | RTX 4090 | RTX 4090 | 24GB | $0.50/hour | Budget |
| **Vast.ai** | Various | Various | Various | $0.20-2.00/hour | Flexible |

### Cloud Setup Scripts

#### Google Colab Setup
```python
# Google Colab setup for Arabic Qwen fine-tuning
!pip install transformers datasets accelerate bitsandbytes peft
!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Check GPU
import torch
print(f"GPU available: {torch.cuda.is_available()}")
print(f"GPU name: {torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None'}")
print(f"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB")

# Mount Google Drive for persistent storage
from google.colab import drive
drive.mount('/content/drive')

# Set up working directory
import os
os.chdir('/content/drive/MyDrive/arabic-qwen-training')
```

#### AWS SageMaker Setup
```python
# AWS SageMaker setup script
import sagemaker
from sagemaker.pytorch import PyTorch

# Define training job
estimator = PyTorch(
    entry_point='train_arabic_qwen.py',
    source_dir='./src',
    role=sagemaker.get_execution_role(),
    instance_type='ml.p3.2xlarge',
    instance_count=1,
    framework_version='2.0.0',
    py_version='py310',
    hyperparameters={
        'model_name': 'Qwen/Qwen2.5-3B',
        'dataset_name': 'FreedomIntelligence/InstAr-500k',
        'output_dir': '/opt/ml/model',
        'num_train_epochs': 3,
        'per_device_train_batch_size': 4,
        'learning_rate': 2e-5
    },
    use_spot_instances=True,  # Save up to 90% on costs
    max_wait=7200,  # 2 hours
    max_run=3600    # 1 hour
)

# Start training
estimator.fit({'training': 's3://your-bucket/arabic-datasets/'})
```

### Cost Optimization Strategies

```python
class CloudCostOptimizer:
    def __init__(self):
        self.spot_savings = 0.7  # 70% savings with spot instances
        self.preemptible_savings = 0.8  # 80% savings with preemptible
        
    def calculate_optimal_cloud_strategy(self, 
                                       training_time_hours: float,
                                       urgency: str = "normal") -> dict:
        """Calculate optimal cloud strategy"""
        
        strategies = {
            "on_demand": {
                "cost_multiplier": 1.0,
                "reliability": 0.99,
                "description": "Standard on-demand instances"
            },
            "spot_instances": {
                "cost_multiplier": 1 - self.spot_savings,
                "reliability": 0.85,
                "description": "AWS Spot instances (can be interrupted)"
            },
            "preemptible": {
                "cost_multiplier": 1 - self.preemptible_savings,
                "reliability": 0.80,
                "description": "GCP Preemptible instances"
            },
            "reserved": {
                "cost_multiplier": 0.6,
                "reliability": 0.99,
                "description": "1-year reserved instances"
            }
        }
        
        base_cost_per_hour = 3.0  # Average cost
        
        recommendations = []
        
        for strategy_name, strategy in strategies.items():
            total_cost = training_time_hours * base_cost_per_hour * strategy["cost_multiplier"]
            
            # Adjust for urgency
            if urgency == "urgent" and strategy["reliability"] < 0.95:
                continue
                
            recommendations.append({
                "strategy": strategy_name,
                "total_cost": total_cost,
                "cost_per_hour": base_cost_per_hour * strategy["cost_multiplier"],
                "reliability": strategy["reliability"],
                "description": strategy["description"],
                "suitable_for_urgency": urgency
            })
        
        # Sort by cost
        recommendations.sort(key=lambda x: x["total_cost"])
        
        return {
            "training_hours": training_time_hours,
            "urgency": urgency,
            "recommendations": recommendations,
            "best_option": recommendations[0] if recommendations else None
        }
    
    def estimate_monthly_cloud_budget(self, 
                                    experiments_per_month: int,
                                    avg_experiment_hours: float) -> dict:
        """Estimate monthly cloud budget"""
        
        total_hours = experiments_per_month * avg_experiment_hours
        
        # Different pricing tiers
        pricing_scenarios = {
            "budget": {
                "cost_per_hour": 0.50,  # RunPod/Vast.ai
                "description": "Budget cloud providers"
            },
            "mainstream": {
                "cost_per_hour": 1.50,  # Lambda Labs
                "description": "Mainstream cloud providers"
            },
            "premium": {
                "cost_per_hour": 3.00,  # AWS/GCP/Azure
                "description": "Premium cloud providers"
            }
        }
        
        budget_estimates = {}
        
        for tier, pricing in pricing_scenarios.items():
            monthly_cost = total_hours * pricing["cost_per_hour"]
            
            budget_estimates[tier] = {
                "monthly_cost": monthly_cost,
                "annual_cost": monthly_cost * 12,
                "cost_per_experiment": monthly_cost / experiments_per_month,
                "description": pricing["description"]
            }
        
        return {
            "experiments_per_month": experiments_per_month,
            "hours_per_experiment": avg_experiment_hours,
            "total_monthly_hours": total_hours,
            "budget_estimates": budget_estimates
        }

# Usage
optimizer = CloudCostOptimizer()

# Calculate optimal strategy for a 6-hour training job
strategy = optimizer.calculate_optimal_cloud_strategy(6, urgency="normal")
print(f"Best strategy: {strategy['best_option']['strategy']}")
print(f"Cost: ${strategy['best_option']['total_cost']:.2f}")
print(f"Reliability: {strategy['best_option']['reliability']*100:.0f}%")

# Estimate monthly budget
budget = optimizer.estimate_monthly_cloud_budget(
    experiments_per_month=10,
    avg_experiment_hours=4
)

print(f"\nMonthly Budget Estimates:")
for tier, estimate in budget["budget_estimates"].items():
    print(f"{tier.capitalize()}: ${estimate['monthly_cost']:.0f}/month (${estimate['cost_per_experiment']:.0f}/experiment)")
```

---

This comprehensive hardware requirements guide provides all the information needed to set up an optimal environment for Arabic Qwen fine-tuning, from budget-conscious setups to high-performance configurations.

## ðŸ“š Next Steps

1. Review [Implementation Examples](./implementation-examples.md) for practical setup
2. Check [Dataset Preparation](./dataset-preparation.md) for data requirements
3. Follow [Fine-tuning Guide](./fine-tuning-guide.md) for training procedures
4. Consult [Troubleshooting Guide](./troubleshooting.md) for hardware issues