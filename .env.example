# Arabic Qwen Base Fine-tuning Environment Configuration
# Copy this file to .env and update the values according to your setup

# =============================================================================
# MODEL CONFIGURATION
# =============================================================================

# Base model to use for fine-tuning
MODEL_NAME=Qwen/Qwen2-7B
# MODEL_NAME=Qwen/Qwen2-1.5B
# MODEL_NAME=Qwen/Qwen2-3B
# MODEL_NAME=Qwen/Qwen2.5-7B

# Model configuration
MAX_LENGTH=2048
USE_FLASH_ATTENTION=true
MODEL_CACHE_DIR=./cache/models

# =============================================================================
# TRAINING CONFIGURATION
# =============================================================================

# Training hyperparameters
BATCH_SIZE=4
GRADIENT_ACCUMULATION_STEPS=4
LEARNING_RATE=5e-5
NUM_EPOCHS=3
MAX_STEPS=1000
WARMUP_STEPS=100
WEIGHT_DECAY=0.01
ADAM_BETA1=0.9
ADAM_BETA2=0.999
ADAM_EPSILON=1e-8
MAX_GRAD_NORM=1.0

# Learning rate scheduler
LR_SCHEDULER_TYPE=cosine
LR_SCHEDULER_WARMUP_RATIO=0.1

# Training behavior
SAVE_STEPS=500
EVAL_STEPS=500
LOGGING_STEPS=10
SAVE_TOTAL_LIMIT=3
LOAD_BEST_MODEL_AT_END=true
METRIC_FOR_BEST_MODEL=eval_loss
GREATER_IS_BETTER=false

# =============================================================================
# DATA CONFIGURATION
# =============================================================================

# Dataset settings
DATASET_NAME=arabic_instructions
# DATASET_NAME=FreedomIntelligence/InstAr-500k
# DATASET_NAME=HuggingFaceH4/ultrachat_200k

TRAIN_SPLIT=train
VALIDATION_SPLIT=validation
TEST_SPLIT=test
MAX_SAMPLES=10000
MAX_EVAL_SAMPLES=1000
DATA_CACHE_DIR=./cache/data

# Data processing
NUM_PROC=4
REMOVE_UNUSED_COLUMNS=true
DATALOADER_NUM_WORKERS=4
DATALOADER_PIN_MEMORY=true

# =============================================================================
# OPTIMIZATION CONFIGURATION
# =============================================================================

# LoRA (Low-Rank Adaptation) settings
USE_LORA=true
LORA_RANK=16
LORA_ALPHA=32
LORA_DROPOUT=0.1
LORA_TARGET_MODULES=q_proj,v_proj,k_proj,o_proj,gate_proj,up_proj,down_proj
LORA_BIAS=none
LORA_TASK_TYPE=CAUSAL_LM

# Quantization settings
USE_QUANTIZATION=true
QUANTIZATION_BITS=4
QUANTIZATION_TYPE=nf4
USE_DOUBLE_QUANT=true
QUANT_COMPUTE_DTYPE=float16

# Memory optimization
GRADIENT_CHECKPOINTING=true
DATALOADER_DROP_LAST=true
FP16=true
BF16=false

# =============================================================================
# PREFERENCE OPTIMIZATION CONFIGURATION
# =============================================================================

# DPO (Direct Preference Optimization) settings
DPO_BETA=0.1
DPO_LOSS_TYPE=sigmoid
DPO_LABEL_SMOOTHING=0.0
DPO_REFERENCE_FREE=false

# KTO (Kahneman-Tversky Optimization) settings
KTO_BETA=0.1
KTO_DESIRABLE_WEIGHT=1.0
KTO_UNDESIRABLE_WEIGHT=1.0

# IPO (Identity Preference Optimization) settings
IPO_BETA=0.1
IPO_TAU=0.1

# CPO (Conservative Preference Optimization) settings
CPO_BETA=0.1
CPO_ALPHA=1.0

# =============================================================================
# EVALUATION CONFIGURATION
# =============================================================================

# Evaluation settings
EVALUATION_STRATEGY=steps
EVAL_ACCUMULATION_STEPS=1
PER_DEVICE_EVAL_BATCH_SIZE=4
DO_EVAL=true
DO_PREDICT=false

# Generation settings for evaluation
GENERATION_MAX_LENGTH=512
GENERATION_NUM_BEAMS=1
GENERATION_DO_SAMPLE=true
GENERATION_TEMPERATURE=0.7
GENERATION_TOP_P=0.9
GENERATION_TOP_K=50

# =============================================================================
# MONITORING AND LOGGING
# =============================================================================

# Weights & Biases
USE_WANDB=true
WANDB_PROJECT=arabic-qwen-finetuning
WANDB_ENTITY=your-team
WANDB_RUN_NAME=
WANDB_TAGS=arabic,qwen,finetuning
WANDB_NOTES=

# TensorBoard
USE_TENSORBOARD=true
TENSORBOARD_LOG_DIR=./logs/tensorboard

# MLflow
USE_MLFLOW=false
MLFLOW_TRACKING_URI=http://localhost:5000
MLFLOW_EXPERIMENT_NAME=arabic-qwen-finetuning

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/training.log
LOG_FORMAT=%(asctime)s - %(name)s - %(levelname)s - %(message)s

# =============================================================================
# PATHS AND DIRECTORIES
# =============================================================================

# Output directories
OUTPUT_DIR=./checkpoints
LOGGING_DIR=./logs
CACHE_DIR=./cache
REPORTS_DIR=./reports

# Model and tokenizer paths
TOKENIZER_NAME=${MODEL_NAME}
TOKENIZER_CACHE_DIR=${MODEL_CACHE_DIR}

# =============================================================================
# HARDWARE AND PERFORMANCE
# =============================================================================

# Device settings
DEVICE=auto
CUDA_VISIBLE_DEVICES=0
# CUDA_VISIBLE_DEVICES=0,1,2,3  # For multi-GPU training

# Performance settings
TORCH_COMPILE=false
TORCH_COMPILE_MODE=default
USE_CPU=false
MPS_ENABLED=false  # For Apple Silicon

# Distributed training
DDP_BACKEND=nccl
DDP_FIND_UNUSED_PARAMETERS=false
DATAPARALLEL=false

# =============================================================================
# SECURITY AND AUTHENTICATION
# =============================================================================

# Hugging Face Hub
HF_TOKEN=your_huggingface_token_here
HF_HUB_CACHE=${CACHE_DIR}/huggingface
HF_HOME=${CACHE_DIR}/huggingface

# API keys (if using external services)
OPENAI_API_KEY=your_openai_api_key_here
ANTHROPIC_API_KEY=your_anthropic_api_key_here

# =============================================================================
# EXPERIMENT CONFIGURATION
# =============================================================================

# Experiment settings
EXPERIMENT_NAME=arabic_qwen_experiment
EXPERIMENT_DESCRIPTION="Fine-tuning Qwen model on Arabic data"
EXPERIMENT_TAGS=arabic,qwen,sft,dpo

# Reproducibility
SEED=42
DETERMINISTIC=true
BENCHMARK=false

# =============================================================================
# ADVANCED CONFIGURATION
# =============================================================================

# Memory management
EMPTY_CACHE_STEPS=100
MAX_MEMORY_MB=
CPU_OFFLOAD=false

# Debugging
DEBUG=false
VERBOSE=false
PROFILE=false
TRACE=false

# Custom configuration files
CUSTOM_CONFIG_PATH=
OVERRIDE_CONFIG=false

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================

# Model serving
SERVING_HOST=0.0.0.0
SERVING_PORT=8000
SERVING_WORKERS=1
SERVING_TIMEOUT=60

# API configuration
API_RATE_LIMIT=100
API_MAX_TOKENS=2048
API_TEMPERATURE=0.7
API_TOP_P=0.9

# =============================================================================
# CLOUD AND STORAGE CONFIGURATION
# =============================================================================

# AWS S3
AWS_ACCESS_KEY_ID=your_aws_access_key
AWS_SECRET_ACCESS_KEY=your_aws_secret_key
AWS_DEFAULT_REGION=us-east-1
S3_BUCKET=your-s3-bucket

# Google Cloud Storage
GOOGLE_APPLICATION_CREDENTIALS=path/to/service-account.json
GCS_BUCKET=your-gcs-bucket

# Azure Blob Storage
AZURE_STORAGE_CONNECTION_STRING=your_azure_connection_string
AZURE_CONTAINER_NAME=your-container

# =============================================================================
# DEVELOPMENT AND TESTING
# =============================================================================

# Development settings
DEV_MODE=false
FAST_DEV_RUN=false
OVERFIT_BATCHES=0
LIMIT_TRAIN_BATCHES=1.0
LIMIT_VAL_BATCHES=1.0
LIMIT_TEST_BATCHES=1.0

# Testing
TEST_MODE=false
TEST_BATCH_SIZE=1
TEST_MAX_SAMPLES=100

# =============================================================================
# CUSTOM EXTENSIONS
# =============================================================================

# Custom hooks and callbacks
CUSTOM_CALLBACKS=
CUSTOM_METRICS=
CUSTOM_OPTIMIZERS=
CUSTOM_SCHEDULERS=

# Plugin configuration
PLUGINS_ENABLED=false
PLUGINS_PATH=./plugins

# =============================================================================
# NOTES
# =============================================================================

# Configuration notes:
# 1. Set USE_LORA=true for memory-efficient training
# 2. Adjust BATCH_SIZE and GRADIENT_ACCUMULATION_STEPS based on your GPU memory
# 3. Use QUANTIZATION for further memory reduction
# 4. Set appropriate MAX_LENGTH based on your data and memory constraints
# 5. Configure monitoring tools (wandb, tensorboard) for experiment tracking
# 6. Ensure proper authentication tokens for external services
# 7. Adjust paths according to your system setup