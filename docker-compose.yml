# Arabic Qwen Base Fine-tuning Docker Compose Configuration
# Provides multi-service setup for development, training, and deployment

version: '3.8'

services:
  # =============================================================================
  # Development Service - Interactive development environment
  # =============================================================================
  dev:
    build:
      context: .
      target: development
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-1.0.0}
    image: arabic-qwen-dev:latest
    container_name: arabic-qwen-dev
    hostname: arabic-qwen-dev
    restart: unless-stopped
    stdin_open: true
    tty: true
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app
      - WANDB_MODE=offline
    env_file:
      - .env
    volumes:
      - .:/app
      - ./cache:/app/cache
      - ./logs:/app/logs
      - ./checkpoints:/app/checkpoints
      - ./reports:/app/reports
      - ./data:/app/data
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8000:8000"  # API server
      - "8888:8888"  # Jupyter
      - "6006:6006"  # TensorBoard
    networks:
      - arabic-qwen-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: bash

  # =============================================================================
  # Training Service - Model training environment
  # =============================================================================
  train:
    build:
      context: .
      target: training
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-1.0.0}
    image: arabic-qwen-train:latest
    container_name: arabic-qwen-train
    hostname: arabic-qwen-train
    restart: "no"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app
      - TORCH_CUDA_ARCH_LIST=6.0;6.1;7.0;7.5;8.0;8.6+PTX
      - CUDA_LAUNCH_BLOCKING=1
    env_file:
      - .env
    volumes:
      - ./config:/app/config:ro
      - ./data:/app/data:ro
      - ./cache:/app/cache
      - ./logs:/app/logs
      - ./checkpoints:/app/checkpoints
      - ./reports:/app/reports
      - ~/.cache/huggingface:/root/.cache/huggingface
    networks:
      - arabic-qwen-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    command: >
      bash -c "
        echo 'Starting training service...' &&
        python -m src.scripts.train --config config/sft_config.yaml
      "
    profiles:
      - training

  # =============================================================================
  # Inference Service - Model serving API
  # =============================================================================
  api:
    build:
      context: .
      target: inference
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-1.0.0}
    image: arabic-qwen-api:latest
    container_name: arabic-qwen-api
    hostname: arabic-qwen-api
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app
      - SERVING_HOST=0.0.0.0
      - SERVING_PORT=8000
    env_file:
      - .env
    volumes:
      - ./checkpoints:/app/models:ro
      - ./cache:/app/cache
      - ./logs:/app/logs
    ports:
      - "8000:8000"
    networks:
      - arabic-qwen-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    depends_on:
      - redis
    profiles:
      - production
      - api

  # =============================================================================
  # Jupyter Service - Interactive notebooks
  # =============================================================================
  jupyter:
    build:
      context: .
      target: jupyter
      args:
        BUILD_DATE: ${BUILD_DATE:-$(date -u +'%Y-%m-%dT%H:%M:%SZ')}
        VCS_REF: ${VCS_REF:-$(git rev-parse --short HEAD)}
        VERSION: ${VERSION:-1.0.0}
    image: arabic-qwen-jupyter:latest
    container_name: arabic-qwen-jupyter
    hostname: arabic-qwen-jupyter
    restart: unless-stopped
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
      - PYTHONPATH=/app
      - JUPYTER_ENABLE_LAB=yes
    env_file:
      - .env
    volumes:
      - .:/app
      - ./notebooks:/app/notebooks
      - ./cache:/app/cache
      - ./logs:/app/logs
      - ./checkpoints:/app/checkpoints
      - ./data:/app/data
      - ~/.cache/huggingface:/root/.cache/huggingface
    ports:
      - "8888:8888"
    networks:
      - arabic-qwen-network
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    profiles:
      - jupyter
      - research

  # =============================================================================
  # TensorBoard Service - Training visualization
  # =============================================================================
  tensorboard:
    image: tensorflow/tensorflow:latest-gpu
    container_name: arabic-qwen-tensorboard
    hostname: arabic-qwen-tensorboard
    restart: unless-stopped
    volumes:
      - ./logs/tensorboard:/logs:ro
    ports:
      - "6006:6006"
    networks:
      - arabic-qwen-network
    command: tensorboard --logdir=/logs --host=0.0.0.0 --port=6006
    profiles:
      - monitoring
      - tensorboard

  # =============================================================================
  # MLflow Service - Experiment tracking
  # =============================================================================
  mlflow:
    image: python:3.9-slim
    container_name: arabic-qwen-mlflow
    hostname: arabic-qwen-mlflow
    restart: unless-stopped
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow/mlflow.db
      - MLFLOW_DEFAULT_ARTIFACT_ROOT=/mlflow/artifacts
    volumes:
      - ./mlflow:/mlflow
    ports:
      - "5000:5000"
    networks:
      - arabic-qwen-network
    command: >
      bash -c "
        pip install mlflow &&
        mlflow server \
          --backend-store-uri sqlite:///mlflow/mlflow.db \
          --default-artifact-root /mlflow/artifacts \
          --host 0.0.0.0 \
          --port 5000
      "
    profiles:
      - monitoring
      - mlflow

  # =============================================================================
  # Redis Service - Caching and session storage
  # =============================================================================
  redis:
    image: redis:7-alpine
    container_name: arabic-qwen-redis
    hostname: arabic-qwen-redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - arabic-qwen-network
    command: redis-server --appendonly yes
    profiles:
      - production
      - api
      - cache

  # =============================================================================
  # PostgreSQL Service - Metadata and experiment storage
  # =============================================================================
  postgres:
    image: postgres:15-alpine
    container_name: arabic-qwen-postgres
    hostname: arabic-qwen-postgres
    restart: unless-stopped
    environment:
      - POSTGRES_DB=arabic_qwen
      - POSTGRES_USER=arabic_qwen
      - POSTGRES_PASSWORD=arabic_qwen_password
      - PGDATA=/var/lib/postgresql/data/pgdata
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init_db.sql:/docker-entrypoint-initdb.d/init_db.sql:ro
    ports:
      - "5432:5432"
    networks:
      - arabic-qwen-network
    profiles:
      - database
      - production

  # =============================================================================
  # Nginx Service - Reverse proxy and load balancer
  # =============================================================================
  nginx:
    image: nginx:alpine
    container_name: arabic-qwen-nginx
    hostname: arabic-qwen-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - ./nginx/ssl:/etc/nginx/ssl:ro
    networks:
      - arabic-qwen-network
    depends_on:
      - api
    profiles:
      - production
      - proxy

  # =============================================================================
  # Prometheus Service - Metrics collection
  # =============================================================================
  prometheus:
    image: prom/prometheus:latest
    container_name: arabic-qwen-prometheus
    hostname: arabic-qwen-prometheus
    restart: unless-stopped
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - arabic-qwen-network
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    profiles:
      - monitoring
      - prometheus

  # =============================================================================
  # Grafana Service - Metrics visualization
  # =============================================================================
  grafana:
    image: grafana/grafana:latest
    container_name: arabic-qwen-grafana
    hostname: arabic-qwen-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources:ro
    networks:
      - arabic-qwen-network
    depends_on:
      - prometheus
    profiles:
      - monitoring
      - grafana

# =============================================================================
# Networks
# =============================================================================
networks:
  arabic-qwen-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16

# =============================================================================
# Volumes
# =============================================================================
volumes:
  redis_data:
    driver: local
  postgres_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

# =============================================================================
# Usage Examples:
# 
# Development setup:
# docker-compose up dev
# 
# Training setup:
# docker-compose --profile training up train
# 
# Production API:
# docker-compose --profile production up -d
# 
# Research environment:
# docker-compose --profile research up jupyter
# 
# Full monitoring stack:
# docker-compose --profile monitoring up -d
# 
# Complete development environment:
# docker-compose --profile development --profile monitoring up -d
# 
# Scale API service:
# docker-compose --profile production up -d --scale api=3
# 
# View logs:
# docker-compose logs -f api
# 
# Execute commands in running container:
# docker-compose exec dev bash
# docker-compose exec train python -m src.scripts.evaluate
# 
# Stop all services:
# docker-compose down
# 
# Stop and remove volumes:
# docker-compose down -v
# 
# Build and start specific service:
# docker-compose build dev && docker-compose up dev
# =============================================================================